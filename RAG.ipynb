{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoTtZ1SAq-zc"
      },
      "outputs": [],
      "source": [
        "pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "JFFcfpqWrKbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "AoQOB9kXryf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "id": "PxrIeus0sdmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "Fq9lvn3exQas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,DocumentSummaryIndex\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "JTS2jzAhspGc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=SimpleDirectoryReader('/content/drive/MyDrive/data').load_data()\n",
        "documents"
      ],
      "metadata": {
        "id": "VJBle9tat22_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt='''\n",
        "You area chatbot and give answer when asked\n",
        "'''\n",
        "# a format supported by llama2\n",
        "format_wrapper=SimpleInputPrompt('<|USER|>{query_str}<|ASSISTANT|>')"
      ],
      "metadata": {
        "id": "ZqAxYIajzEj_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFglCDX44PT6",
        "outputId": "458395ea-6e74-4fce-df87-1f3c83026e28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm_model = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=format_wrapper,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # optional\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")\n"
      ],
      "metadata": {
        "id": "FotGcqYH4cJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-langchain"
      ],
      "metadata": {
        "id": "v_IRGWTiBSVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n"
      ],
      "metadata": {
        "id": "Z-pCrZ4n5zqs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents,embed_model=embed_model)"
      ],
      "metadata": {
        "id": "s7qRVm0lB4tC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVuBpqWgrO8N",
        "outputId": "61fcb2f8-da8d-466f-d690-de03fd633cdd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7fe2fa7c7f40>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine(llm=llm_model)\n",
        "response1=query_engine.query(\"How to reshape data?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onHs5vDnre3D",
        "outputId": "8f76b4f4-3e2e-4164-ace4-f41875cab7ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsku88BPsRZD",
        "outputId": "a77f70fe-3a06-4e1f-ce26-f0c1f4552b54"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "There are several ways to reshape data in pandas, depending on the desired outcome. Here are some common methods:\n",
            "\n",
            "1. **Gather columns into rows**: Use the `pivot()` method to turn columns into rows. For example, `df.pivot(columns='var', values='val')` will reshape a DataFrame with columns `var` and values `val` into a DataFrame with rows `var` and values `val`.\n",
            "2. **Spread rows into columns**: Use the `melt()` method to turn rows into columns. For example, `df.melt(id_vars=['Length', 'Width'], var_name='Species')` will reshape a DataFrame with rows and columns into a DataFrame with columns and rows.\n",
            "3. **Append rows of DataFrames**: Use the `concat()` method to append rows of DataFrames. For example, `df1 = pd.DataFrame({'Length': [10], 'Width': [2]})` and `df2 = pd.DataFrame({'Length': [5], 'Width': [3]})` will reshape two DataFrames into a single DataFrame with rows.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2=query_engine.query('What is numpy?')\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSiOfCmZshWt",
        "outputId": "8658f290-7b53-4111-8db9-720d2f1a6c07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "numpy is a library for numerical computing. It provides support for large, multi-dimensional arrays and matrices, and a wide range of high-level mathematical functions to manipulate them. It is widely used in data science, machine learning, and scientific computing.\n",
            "\n",
            "Please let me know if you have any other questions or if there is anything else I can help you with.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = DocumentSummaryIndex.from_documents(\n",
        "    documents, embed_model=embed_model, llm=llm_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps8VXEeRuM4V",
        "outputId": "a8af0794-1a41-49bf-953a-c2fe490b1a6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current doc id: 5671cece-74c1-483e-96ff-1e53db0b1f35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current doc id: a31f9afa-4ec8-4582-86a2-0e3ffad926f9\n",
            "current doc id: ee5f72f3-c434-4974-a1c2-092ff82bd80a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3=query_engine.query('What are the different ways of handling missing data?')\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBi_cGIxwgaE",
        "outputId": "7413c192-ddfc-4009-b20f-23bdc35ae170"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "There are several ways to handle missing data in pandas, including:\n",
            "\n",
            "1. Dropna() - This function drops rows with any column having NA/null data.\n",
            "2. Fillna(value) - This function replaces all NA/null data with a specified value.\n",
            "3. Dropna() and fillna() can be combined to drop rows with missing data and fill them with a specific value.\n",
            "4. Select_na() - This function selects the rows with missing data and returns them as a Series.\n",
            "5. Replace() - This function replaces the missing data with a specified value.\n",
            "6. Impute() - This function imputes the missing data using a specified method.\n",
            "7. Fill() - This function fills the missing data using a specified method.\n",
            "8. Complete() - This function completes the missing data using a specified method.\n",
            "9. Missing() - This function returns the missing data as a Series.\n",
            "10. Na() - This function returns the missing data as a Series.\n",
            "\n",
            "These are some of the ways to handle missing data in pandas, and the choice of method depends on the specific use case and the type of data being worked with.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bhtnx3hfxMkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}